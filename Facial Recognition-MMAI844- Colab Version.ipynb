{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MMAI 844 Facial Recognition Demo - Colab Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: left; width: 75%; height: 200px; padding-bottom:400px\">\n",
    "    <img src=\"https://www.digitalvidya.com/wp-content/uploads/2018/09/Face-Recognition-Python-1280x720.jpg\" alt=\"CNN\">\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/BenchengW/Facial-Recognition-MMAI844-Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install keras-vggface\n",
    "! pip install keras_preprocessing \n",
    "! pip install keras_applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facial Image Classification with Tensorflow pre-trained model\n",
    "\n",
    "Face recognition is the general task of identifying and verifying people from photographs of their face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Imports Libraries\n",
    "Import libraries and define environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "print(cv2.__version__)\n",
    "%matplotlib inline\n",
    "cv2.startWindowThread()\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras_vggface\n",
    "from keras.engine import  Model\n",
    "from keras.layers import Flatten, Dense, Input\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras_vggface.utils import preprocess_input\n",
    "from keras_vggface.utils import decode_predictions\n",
    "from numpy import asarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: left; width: 75%; height: 200px; padding-bottom:350px\">\n",
    "    <img src=\"https://lh3.googleusercontent.com/IdcOyMJ4hCDvSJXWBo1Rxr1BTM9fQWoxShs0tdS93bpyQ1K6vIog_mV9LrfE0DwKK61X2fHY51AAbPJTkOOMDUVxaiE32JsGog74k3lnXKXPefpd_fSC3divPG3AEEQhaith6S47\" alt=\"CNN\">\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: Load Data/Image\n",
    "#### Read and Write Images\n",
    "``` python\n",
    "cv2.imwrite(file_path (str), image (numpy.ndarray))\n",
    "cv2.imread(file_path (str), read_mode (int))```\n",
    "#### Read Modes\n",
    "-  ```1 = cv2.IMREAD_COLOR```\n",
    "-  ```0 = cv2.IMREAD_GRAYSCALE```\n",
    "- ```-1 = cv2.IMREAD_UNCHANGED```\n",
    "\n",
    "Load a Sample Simith image using opencv library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img =cv2.imread(\"/content/Facial-Recognition-MMAI844-Tutorial/sample/Smith.jpg\",1)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also Take a Picture or start video with your webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_show(image, title=\"\"):\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.imshow(image, cmap=\"Greys_r\")\n",
    "    plt.show()\n",
    "    \n",
    "########################################\n",
    "#Take a Picture when you camera is free\n",
    "#########################################\n",
    "webcam = cv2.VideoCapture(0)\n",
    "ret, frame = webcam.read()\n",
    "print(ret)\n",
    "webcam.release()\n",
    "if ret:\n",
    "    plt_show(frame)\n",
    "else:\n",
    "    print(\"Camera is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.3: Detecting Faces with OpenCV and front face detector xml \n",
    "``` python\n",
    "detector = cv2.CascadeClassifier( xml_file_path)\n",
    "face_coord = detector.detectMultiScale(image, scale_factor, min_neighbors, min_size, flags)\n",
    "```\n",
    "face_coord: Numpy array with rows equal to [x, y, width, height]\n",
    "\n",
    "Let import a image to detect faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame= cv2.imread('/content/Facial-Recognition-MMAI844-Tutorial/sample/channing_tatum.jpg')\n",
    "name= \"channing_tatum\"\n",
    "plt_show(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = cv2.CascadeClassifier(\"/content/Facial-Recognition-MMAI844-Tutorial/xml/frontal_face.xml\")\n",
    "\n",
    "scale_factor = 1.2\n",
    "min_neighbors = 5\n",
    "min_size = (40, 40)\n",
    "biggest_only = True\n",
    "flags = cv2.CASCADE_FIND_BIGGEST_OBJECT | \\\n",
    "            cv2.CASCADE_DO_ROUGH_SEARCH if biggest_only else \\\n",
    "            cv2.CASCADE_SCALE_IMAGE\n",
    "        \n",
    "faces_coord = detector.detectMultiScale(frame,\n",
    "                                        scaleFactor=scale_factor,\n",
    "                                        minNeighbors=min_neighbors,\n",
    "                                        minSize=min_size,\n",
    "                                        flags=flags)\n",
    "print(\"Type: \" + str(type(faces_coord)))\n",
    "print(\"this is face coordinator in the picture is {}\".format(faces_coord))\n",
    "print(\"Face is successfully detected!! Let draw a box on the picture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_rectangle(image, coords):\n",
    "    for (x, y, w, h) in coords:\n",
    "        w_rm = int(0.2 * w / 2) \n",
    "        cv2.rectangle(image, (x + w_rm, y), (x + w - w_rm, y + h), \n",
    "                              (0, 0, 255), 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_rectangle(frame,faces_coord)\n",
    "cv2.putText(frame, name,    \n",
    "                            (faces_coord[0][0], faces_coord[0][1]),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX , 2, (66, 53, 243), 6)\n",
    "plt_show(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrap up the code into a face detector module\n",
    "class FaceDetector(object):\n",
    "    def __init__(self, xml_path):\n",
    "        self.classifier = cv2.CascadeClassifier(xml_path)\n",
    "    \n",
    "    def detect(self, image, biggest_only=True):\n",
    "        scale_factor = 1.2\n",
    "        min_neighbors = 5\n",
    "        min_size = (30, 30)\n",
    "        biggest_only = True\n",
    "        flags = cv2.CASCADE_FIND_BIGGEST_OBJECT | \\\n",
    "                    cv2.CASCADE_DO_ROUGH_SEARCH if biggest_only else \\\n",
    "                    cv2.CASCADE_SCALE_IMAGE\n",
    "        faces_coord = self.classifier.detectMultiScale(image,\n",
    "                                                       scaleFactor=scale_factor,\n",
    "                                                       minNeighbors=min_neighbors,\n",
    "                                                       minSize=min_size,\n",
    "                                                       flags=flags)\n",
    "        return faces_coord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.4: Cut Faces and resize faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for (x, y, w, h) in faces_coord:\n",
    "    cv2.rectangle(frame, (x, y), (x + w, y + h), (150, 150, 0), 8)\n",
    "plt_show(frame) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_faces(image, faces_coord):\n",
    "    faces = []\n",
    "      \n",
    "    for (x, y, w, h) in faces_coord:\n",
    "        w_rm = int(0.2 * w / 2)\n",
    "        faces.append(image[y: y + h, x + w_rm: x + w - w_rm])\n",
    "         \n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cut_Face = cut_faces(frame, faces_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_show(Cut_Face[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(images, size=(224, 224)):\n",
    "    images_norm = []\n",
    "    for image in images:\n",
    "        if image.shape < size:\n",
    "            image_norm = cv2.resize(image, size, \n",
    "                                    interpolation = cv2.INTER_AREA)\n",
    "        else:\n",
    "            image_norm = cv2.resize(image, size, \n",
    "                                    interpolation = cv2.INTER_CUBIC)\n",
    "        images_norm.append(image_norm)\n",
    "\n",
    "    return images_norm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_faces = resize(Cut_Face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_show(resize_faces[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.5: Standerdize the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Resize_face = resize_faces[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Resize_face/225.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2:Pre-trained Model in Tensorflow\n",
    "\n",
    "### Step 2.1 Install libraries\n",
    "\n",
    "VGGFace and VGGFace2 Models\n",
    "\n",
    "The VGGFace refers to a series of models developed for face recognition and demonstrated on benchmark computer vision datasets by members of the Visual Geometry Group (VGG) at the University of Oxford."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: left; width: 75%; height: 200px; padding-bottom:280px\">\n",
    "    <img src=\"https://pbs.twimg.com/media/Dn2GdPfW0AE0nf7?format=jpg&name=4096x4096\" alt=\"CNN\">\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show keras-vggface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Verify Data and Predict the image using Pre-train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy import asarray\n",
    "# # extract a single face from a given photograph\n",
    "def extract_face_from_file(filename, required_size=(224, 224)):\n",
    "    # load image from file\n",
    "    pixels = plt.imread(filename)\n",
    "    detector = FaceDetector(\"/content/Facial-Recognition-MMAI844-Tutorial/xml/frontal_face.xml\")\n",
    "    # create the detector, using default weights\n",
    "    faces_coord = detector.detect(image=pixels)\n",
    "    faces = cut_faces(pixels, faces_coord)\n",
    "    faces = resize(faces)\n",
    "    return faces[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample= plt.imread('/content/Facial-Recognition-MMAI844-Tutorial/sharon_stone1.jpg')\n",
    "plt.imshow(Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the photo and extract the face\n",
    "extract_face = extract_face_from_file('/content/Facial-Recognition-MMAI844-Tutorial/sample/sharon_stone1.jpg')\n",
    "# plot the extracted face\n",
    "plt.imshow(extract_face)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2 Check Model Input and output, and model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_vggface.vggface import VGGFace\n",
    "# create a vggface2 model\n",
    "model = VGGFace(model='resnet50')\n",
    "# summarize input and output shape\n",
    "print('Inputs: %s' % model.inputs)\n",
    "print('Outputs: %s' % model.outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model expects input color images of faces with the shape of 244×244 and the output will be a class prediction of 8,631 people. the input dimension is 4. This means that you have to reshape your training set with .reshape(n_images, 286, 384, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#prints out summary of model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2 Prepare input for faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Face_array = asarray(extract_face,'float32')\n",
    "Preprocess_face = preprocess_input(Face_array, version=2)\n",
    "print(Preprocess_face.shape)\n",
    "Preprocess_face_input = Preprocess_face.reshape(1, 224, 224, 3)\n",
    "print(Preprocess_face_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform prediction\n",
    "yhat = model.predict(Preprocess_face_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert prediction into names\n",
    "results = decode_predictions(yhat)\n",
    "# display most likely results\n",
    "for result in results[0]:\n",
    "    print('%s: %.3f%%' % (result[0], result[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_rectangle_with_label(image, label):\n",
    "    faces_coord = detector.detectMultiScale(image,\n",
    "                                        scaleFactor=scale_factor,\n",
    "                                        minNeighbors=min_neighbors,\n",
    "                                        minSize=min_size,\n",
    "                                        flags=flags)\n",
    "    draw_rectangle(image,faces_coord)\n",
    "    cv2.putText(image, name,    \n",
    "                            (faces_coord[0][0], faces_coord[0][1]),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX , 1, (66, 53, 243), 3)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    for result in results[0]:\n",
    "        print('%s: %.3f%%' % (result[0], result[1]*100))\n",
    "\n",
    "draw_rectangle_with_label(Sample, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Use Pre-trained Model for Your Photo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are going to use a public dataset for training our own model. \n",
    "You can download the dataset from here: https://www.kaggle.com/dansbecker/5-celebrity-faces-dataset\n",
    "\n",
    "This is a small dataset for experimenting with computer vision techniques. It has a training directory containing 14-20 photos each of the celebrities\n",
    "\n",
    "--Ben Afflek\n",
    "\n",
    "--Elton John\n",
    "\n",
    "--Jerry Seinfeld\n",
    "\n",
    "--Madonna\n",
    "\n",
    "--Mindy Kaling\n",
    "    \n",
    "The validation directory has 5 photos of each celebrity.\n",
    "\n",
    "The photos haven't been cropped for consistent aspect ratios. With so few training photos, this an especially interesting test of computer vision techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pixels = plt.imread(\"/content/Facial-Recognition-MMAI844-Tutorial/train/ben_afflek/httpwwwhillsindcomstorebenjpg.jpg\")\n",
    "plt.imshow(pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1 Prepare input data and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In keras there is a image preprocessing function that can not only do the image preprocessing but also run data augmentation for you.\n",
    "For more information please refer to: https://keras.io/api/preprocessing/image/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height=224\n",
    "img_width=224\n",
    "batch_size=16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2 Specify the data folder train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    zoom_range = 0.1, # Randomly zoom image \n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    #shear_range=0.2,\n",
    "    vertical_flip=False,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '/content/Facial-Recognition-MMAI844-Tutorial/train',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    '/content/Facial-Recognition-MMAI844-Tutorial/val',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom parameters\n",
    "nb_class = 5\n",
    "hidden_dim = 200\n",
    "nb_train_samples = 93\n",
    "\n",
    "nb_validation_samples = 25\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "numclasses = 5\n",
    "\n",
    "vgg_model = VGGFace(include_top=False, input_shape=(224, 224, 3))\n",
    "last_layer = vgg_model.get_layer('pool5').output\n",
    "x = Flatten(name='flatten')(last_layer)\n",
    "x = Dense(hidden_dim, activation='relu', name='fc6')(x)\n",
    "x = Dense(hidden_dim, activation='relu', name='fc7')(x)\n",
    "out = Dense(nb_class, activation='softmax', name='fc8')(x)\n",
    "custom_vgg_model = Model(vgg_model.input, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "custom_vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "decay = 1e-7 #0.0\n",
    "optimizer = RMSprop(lr=lr, decay=decay)\n",
    "custom_vgg_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3 Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = custom_vgg_model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.4 Visualize the trainning and validation accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test loss histories\n",
    "training_loss = history.history['loss']\n",
    "training_acc = history.history['accuracy']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "fig=plt.figure(figsize=(12, 4))\n",
    "# Visualize loss history\n",
    "fig.add_subplot(121)\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, training_acc, 'b-')\n",
    "plt.legend(['Training Loss', 'Training Accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss/Acc')\n",
    "\n",
    "# Get training and test loss histories\n",
    "val_acc = history.history['val_accuracy']\n",
    "training_acc = history.history['accuracy']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(val_acc) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "fig.add_subplot(122)\n",
    "plt.plot(epoch_count, val_acc, 'r--')\n",
    "plt.plot(epoch_count, training_acc, 'b-')\n",
    "plt.legend(['Validation Accuracy', 'Training Accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.5 Save the model and run prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveweight =  'celebriytag_weight.h5'\n",
    "model.save_weights(saveweight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['ben_afflek',  'elton_john',  'jerry_seinfeld',  'madonna',  'mindy_kaling']\n",
    "test_imgs = ['/content/Facial-Recognition-MMAI844-Tutorial/val/ben_afflek/123MTENDgMDUODczNDcNTcjpg.jpg']\n",
    "\n",
    "\n",
    "test_img = '/content/Facial-Recognition-MMAI844-Tutorial/val/ben_afflek/123MTENDgMDUODczNDcNTcjpg.jpg'\n",
    "img = image.load_img(test_img, target_size=(img_width, img_height))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x /= 255.\n",
    "classes = custom_vgg_model.predict(x)\n",
    "result = np.squeeze(classes)\n",
    "result_indices = np.argmax(result)\n",
    "    \n",
    "img = cv2.imread(test_img, cv2.IMREAD_COLOR)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.axis('off')\n",
    "plt.title(\"{}, {:.2f}%\".format(labels[result_indices], result[result_indices]*100))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
