{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial - Facial_Recognition_MMAI844_Colab_Version.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgmaT2_vo5oV"
      },
      "source": [
        "## MMAI 844 Facial Recognition Demo - Colab Version\n",
        "\n",
        "## Team Albert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6vO1pX_o5oV"
      },
      "source": [
        "<div style=\"float: left; width: 75%; height: 200px; padding-bottom:400px\">\n",
        "    <img src=\"https://www.digitalvidya.com/wp-content/uploads/2018/09/Face-Recognition-Python-1280x720.jpg\" alt=\"CNN\">\n",
        "</div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnhKgaQlo5oV"
      },
      "source": [
        "# Clone Github Repo which includes sample data\n",
        "!git clone https://github.com/BenchengW/Facial-Recognition-MMAI844-Tutorial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huBviVPyo5oV"
      },
      "source": [
        "# Install VGGface for later use\n",
        "! pip install keras-vggface\n",
        "! pip install keras_preprocessing \n",
        "! pip install keras_applications"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nfgnucOo5oW"
      },
      "source": [
        "# Facial Image Classification with Tensorflow pre-trained model\n",
        "\n",
        "Face recognition is the general task of identifying and verifying people from photographs of their face."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK0xsHWAbeoK"
      },
      "source": [
        "<div style=\"float: left; width: 75%; height: 550px; padding-bottom:350px\">\n",
        "    <img src=\"https://pyimagesearch.com/wp-content/uploads/2018/09/opencv_face_reco_facenet.jpg\" alt=\"CNN\">\n",
        "</div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI2_7s_So5oW"
      },
      "source": [
        "## Step 1.1: Importing Libraries\n",
        "Import libraries and define environment variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsD3Ls3wo5oW"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "print(cv2.__version__)\n",
        "%matplotlib inline\n",
        "cv2.startWindowThread()\n",
        "from os import listdir\n",
        "from PIL import Image\n",
        "import warnings\n",
        "warnings.filterwarnings(action='once')\n",
        "import urllib.request"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKCSbFrFo5oY"
      },
      "source": [
        "import keras\n",
        "import keras_vggface\n",
        "from keras.engine import  Model\n",
        "from keras.layers import Flatten, Dense, Input\n",
        "from keras_vggface.vggface import VGGFace\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from keras.optimizers import RMSprop, SGD\n",
        "from keras_vggface.utils import preprocess_input\n",
        "from keras_vggface.utils import decode_predictions\n",
        "from numpy import asarray\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.models import Model\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from keras.preprocessing import image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WRc_l03o5oY"
      },
      "source": [
        "## Step 1.2: Load Data/Image\n",
        "\n",
        "There are many ways to load images using Python, such as OpenCv, MatplotLib, PIL, etc\n",
        "\n",
        "#### Read and Write Images Example  using OpenCV\n",
        "``` python\n",
        "cv2.imwrite(file_path (str), image (numpy.ndarray))\n",
        "cv2.imread(file_path (str), read_mode (int))```\n",
        "#### Read Modes\n",
        "-  ```1 = cv2.IMREAD_COLOR```\n",
        "-  ```0 = cv2.IMREAD_GRAYSCALE```\n",
        "- ```-1 = cv2.IMREAD_UNCHANGED```\n",
        "\n",
        "Load a Sample Smith image using opencv library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQXmdMBlo5oY"
      },
      "source": [
        "# Read Image using OpenCV\n",
        "\n",
        "import cv2\n",
        "\n",
        "img =cv2.imread(\"/content/Facial-Recognition-MMAI844-Tutorial/sample/Smith.jpg\",1)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zQiTsXto5oZ"
      },
      "source": [
        "# Read Image using matplotlib\n",
        "\n",
        "import matplotlib.image as mpimg \n",
        "import matplotlib.pyplot as plt \n",
        "  \n",
        "img = mpimg.imread('/content/Facial-Recognition-MMAI844-Tutorial/sample/Smith.jpg') \n",
        "plt.imshow(img) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJMQuI3Uo5oZ"
      },
      "source": [
        "<div style=\"float: left; width: 75%; height: 550px; padding-bottom:350px\">\n",
        "    <img src=\"https://pyimagesearch.com/wp-content/uploads/2018/09/opencv_face_reco_facenet.jpg\" alt=\"CNN\">\n",
        "</div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh_1iKKTo5oZ"
      },
      "source": [
        "## Step 1.3: Detecting Faces with OpenCV and front face detector xml \n",
        "``` python\n",
        "detector = cv2.CascadeClassifier( xml_file_path)\n",
        "face_coord = detector.detectMultiScale(image, scale_factor, min_neighbors, min_size, flags)\n",
        "```\n",
        "face_coord: Numpy array with rows equal to [x, y, width, height]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6SO_QOlbeoL"
      },
      "source": [
        "<div style=\"float: left; width: 75%; height: 200px; padding-bottom:350px\">\n",
        "    <img src=\"https://lh3.googleusercontent.com/IdcOyMJ4hCDvSJXWBo1Rxr1BTM9fQWoxShs0tdS93bpyQ1K6vIog_mV9LrfE0DwKK61X2fHY51AAbPJTkOOMDUVxaiE32JsGog74k3lnXKXPefpd_fSC3divPG3AEEQhaith6S47\" alt=\"CNN\">\n",
        "</div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsh1mNx2o5oZ"
      },
      "source": [
        "#### Lets import a facial image to detect the face"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag8pNVgZo5oZ"
      },
      "source": [
        "frame= cv2.imread('/content/Facial-Recognition-MMAI844-Tutorial/sample/channing_tatum.jpg')\n",
        "name= \"channing_tatum\"\n",
        "plt_show(frame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NP0Lz5Go5oZ"
      },
      "source": [
        "#### Below is the loading of the OpenCv face detector module and the detectionn of the face coordinators on the image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8-QIxcuo5oa"
      },
      "source": [
        "detector = cv2.CascadeClassifier(\"/content/Facial-Recognition-MMAI844-Tutorial/xml/frontal_face.xml\")\n",
        "\n",
        "scale_factor = 1.2\n",
        "min_neighbors = 5\n",
        "min_size = (40, 40)\n",
        "biggest_only = True\n",
        "flags = cv2.CASCADE_FIND_BIGGEST_OBJECT | \\\n",
        "            cv2.CASCADE_DO_ROUGH_SEARCH if biggest_only else \\\n",
        "            cv2.CASCADE_SCALE_IMAGE\n",
        "        \n",
        "faces_coord = detector.detectMultiScale(frame,\n",
        "                                        scaleFactor=scale_factor,\n",
        "                                        minNeighbors=min_neighbors,\n",
        "                                        minSize=min_size,\n",
        "                                        flags=flags)\n",
        "print(\"Type: \" + str(type(faces_coord)))\n",
        "print(\"this is face coordinator in the picture is {}\".format(faces_coord))\n",
        "print(\"Face is successfully detected!! Let draw a box on the picture\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO_l0y1-o5oa"
      },
      "source": [
        "#### Define a function to draw a rectangle around the detected face"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv5-CB0Zo5oa"
      },
      "source": [
        "def draw_rectangle(image, coords):\n",
        "    for (x, y, w, h) in coords:\n",
        "        w_rm = int(0.2 * w / 2) \n",
        "        cv2.rectangle(image, (x + w_rm, y), (x + w - w_rm, y + h), \n",
        "                              (0, 0, 255), 8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n90VUdNDo5oa"
      },
      "source": [
        "draw_rectangle(frame,faces_coord)\n",
        "cv2.putText(frame, name,    \n",
        "                            (faces_coord[0][0], faces_coord[0][1]),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX , 2, (66, 53, 243), 6)\n",
        "plt_show(frame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJWwrcWWo5ob"
      },
      "source": [
        "############################################################\n",
        "# Wrap up the code into a face detector module for later use\n",
        "############################################################\n",
        "\n",
        "class FaceDetector(object):\n",
        "    def __init__(self, xml_path):\n",
        "        self.classifier = cv2.CascadeClassifier(xml_path)\n",
        "    \n",
        "    def detect(self, image, biggest_only=True):\n",
        "        scale_factor = 1.2\n",
        "        min_neighbors = 5\n",
        "        min_size = (30, 30)\n",
        "        biggest_only = True\n",
        "        flags = cv2.CASCADE_FIND_BIGGEST_OBJECT | \\\n",
        "                    cv2.CASCADE_DO_ROUGH_SEARCH if biggest_only else \\\n",
        "                    cv2.CASCADE_SCALE_IMAGE\n",
        "        faces_coord = self.classifier.detectMultiScale(image,\n",
        "                                                       scaleFactor=scale_factor,\n",
        "                                                       minNeighbors=min_neighbors,\n",
        "                                                       minSize=min_size,\n",
        "                                                       flags=flags)\n",
        "        return faces_coord"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3WVNlR4o5ob"
      },
      "source": [
        "## Step 1.4: Cut Faces and resize faces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blpz-Ngho5ob"
      },
      "source": [
        "<div style=\"float: left; width: 75%; height: 550px; padding-bottom:350px\">\n",
        "    <img src=\"https://i0.wp.com/www.life2coding.com/wp-content/uploads/2018/01/crop.jpg?resize=768%2C628&ssl=1\" alt=\"CNN\">\n",
        "</div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQzTNT1do5ob"
      },
      "source": [
        "for (x, y, w, h) in faces_coord:\n",
        "    cv2.rectangle(frame, (x, y), (x + w, y + h), (150, 150, 0), 8)\n",
        "plt_show(frame) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7GUTOxdo5ob"
      },
      "source": [
        "def cut_faces(image, faces_coord):\n",
        "    faces = []\n",
        "      \n",
        "    for (x, y, w, h) in faces_coord:\n",
        "        w_rm = int(0.2 * w / 2)\n",
        "        faces.append(image[y: y + h, x + w_rm: x + w - w_rm])\n",
        "         \n",
        "    return faces"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwHgG8LCo5ob"
      },
      "source": [
        "Cut_Face = cut_faces(frame, faces_coord)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEuVG3cuo5ob"
      },
      "source": [
        "plt_show(Cut_Face[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWSZsAcjo5ob"
      },
      "source": [
        "#### Define a fuction to resize the face back to 224x224 resolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8icqZk6o5ob"
      },
      "source": [
        "def resize(images, size=(224, 224)):\n",
        "    images_norm = []\n",
        "    for image in images:\n",
        "        if image.shape < size:\n",
        "            image_norm = cv2.resize(image, size, \n",
        "                                    interpolation = cv2.INTER_AREA)\n",
        "        else:\n",
        "            image_norm = cv2.resize(image, size, \n",
        "                                    interpolation = cv2.INTER_CUBIC)\n",
        "        images_norm.append(image_norm)\n",
        "\n",
        "    return images_norm "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nESpmmwZo5ob"
      },
      "source": [
        "resize_faces = resize(Cut_Face)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAIUNW-fo5ob"
      },
      "source": [
        "plt_show(resize_faces[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X_Ofq1Xo5ob"
      },
      "source": [
        "### Step 1.5: Standardize the image for fitting Neural Net Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uhBvMTOo5ob"
      },
      "source": [
        "Normalizing image inputs: Data normalization is an important step which ensures that each input parameter (pixel, in this case) has a similar data distribution. This makes convergence faster while training the network.\n",
        "\n",
        "For image inputs we need the pixel numbers to be positive, so we might choose to scale the normalized data in the range [0,1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUEWi_uurfHa"
      },
      "source": [
        "resize_face_stad = resize_faces[0]/255.0\n",
        "plt.imshow((resize_faces[0]/255))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luzz0Etio5ob"
      },
      "source": [
        "# Pre-trained Model in Tensorflow\n",
        "\n",
        "## Step 2.1 Install libraries\n",
        "\n",
        "VGGFace and VGGFace2 Models\n",
        "\n",
        "The VGGFace refers to a series of models developed for face recognition and demonstrated on benchmark computer vision datasets by members of the Visual Geometry Group (VGG) at the University of Oxford.\n",
        "\n",
        "#### For more information please visit the github link:https://github.com/rcmalli/keras-vggface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI6vqwZzo5ob"
      },
      "source": [
        "<div style=\"float: left; width: 90%; height: 250px; padding-bottom:280px\">\n",
        "    <img src=\"https://pbs.twimg.com/media/Dn2GdPfW0AE0nf7?format=jpg&name=4096x4096\" alt=\"CNN\">\n",
        "</div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArYcx-ELo5oc"
      },
      "source": [
        "## Step 2.1: Verify Data and extract faces from public dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qncRST8Eo5oc"
      },
      "source": [
        "######################################################\n",
        "# Define a function to load the image, then detect faces, and finnally cut face\n",
        "######################################################\n",
        "\n",
        "def extract_face_from_file(filename, required_size=(224, 224)):\n",
        "    # load image from file\n",
        "    pixels = plt.imread(filename)\n",
        "    detector = FaceDetector(\"/content/Facial-Recognition-MMAI844-Tutorial/xml/frontal_face.xml\")\n",
        "    # create the detector, using default weights\n",
        "    faces_coord = detector.detect(image=pixels)\n",
        "    faces = cut_faces(pixels, faces_coord)\n",
        "    faces = resize(faces)\n",
        "    return faces[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7zwAwqZo5oc"
      },
      "source": [
        "print(\"Laoding the sample piciure of Sharon Ston \\n\")\n",
        "\n",
        "Sample= plt.imread('/content/Facial-Recognition-MMAI844-Tutorial/sample/sharon_stone1.jpg')\n",
        "plt.imshow(Sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqZgQccNo5oc"
      },
      "source": [
        "#####################################\n",
        "# load the photo and extract the face\n",
        "#####################################\n",
        "\n",
        "extract_face = extract_face_from_file('/content/Facial-Recognition-MMAI844-Tutorial/sample/sharon_stone1.jpg')\n",
        "plt.imshow(extract_face)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSJbTgblo5oc"
      },
      "source": [
        "## Step 2.2 Check Model Input and output, and model summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiOAqlxFo5oc"
      },
      "source": [
        "from keras_vggface.vggface import VGGFace\n",
        "# create a vggface2 model\n",
        "model = VGGFace(model='resnet50')\n",
        "# summarize input and output shape\n",
        "print('Inputs: %s' % model.inputs)\n",
        "print('Outputs: %s' % model.outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaIbz8bXo5oc"
      },
      "source": [
        "We can see that the model expects input color images of faces with the shape of 244×244 and the output will be a class prediction of 8,631 people. The input dimension is 4. This means that you have to reshape your training set with .reshape(n_images, 286, 384, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WupEuhslo5oc"
      },
      "source": [
        "### prints out summary of model\n",
        "\n",
        "The model expects input color images of faces with the shape of 244×244 and the output will be a class prediction of 8,631 people\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLtAkxpxo5oc",
        "scrolled": true
      },
      "source": [
        "#Printing out summary of model\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d9mQckso5oc"
      },
      "source": [
        "## Step 2.2 Prepare input and predict the image using Pre-train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFcfJeb6o5oc"
      },
      "source": [
        "###########################################\n",
        "# Prepare the input for feeding the model\n",
        "###########################################\n",
        "\n",
        "Face_array = asarray(extract_face,'float32')\n",
        "Preprocess_face = preprocess_input(Face_array)\n",
        "print(Preprocess_face.shape)\n",
        "Preprocess_face_input = Preprocess_face.reshape(1, 224, 224, 3)\n",
        "print(Preprocess_face_input.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntdn8SEBo5oc"
      },
      "source": [
        "#### Run Prediction and convert prediction into names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhV5EE1Go5oc"
      },
      "source": [
        "# perform prediction\n",
        "yhat = model.predict(Preprocess_face_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riqvuteJo5oc"
      },
      "source": [
        "# convert prediction into names\n",
        "results = decode_predictions(yhat)\n",
        "# display most likely results\n",
        "for result in results[0]:\n",
        "    print('%s: %.3f%%' % (result[0], result[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV5njPNro5oc"
      },
      "source": [
        "#### Draw a rectangle on the original image with the prediction label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjJnv_g9o5oc"
      },
      "source": [
        "def draw_rectangle_with_label(image, label):\n",
        "    faces_coord = detector.detectMultiScale(image,\n",
        "                                        scaleFactor=scale_factor,\n",
        "                                        minNeighbors=min_neighbors,\n",
        "                                        minSize=min_size,\n",
        "                                        flags=flags)\n",
        "    draw_rectangle(image,faces_coord)\n",
        "    cv2.putText(image, name,    \n",
        "                            (faces_coord[0][0], faces_coord[0][1]),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX , 1, (66, 53, 243), 3)\n",
        "\n",
        "    plt.imshow(image)\n",
        "    plt.show()\n",
        "    for result in results[0]:\n",
        "        print('%s: %.3f%%' % (result[0], result[1]*100))\n",
        "\n",
        "draw_rectangle_with_label(Sample, results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aXaBRhPo5oc"
      },
      "source": [
        "## Step 2.3 Understand the hidden layer in VGG or CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iS0KhBPo5oc"
      },
      "source": [
        "<div style=\"float: left; width: 75%; height: 200px; padding-bottom:280px\">\n",
        "    <img src=\"http://www.ukherald.com/wp-content/uploads/2015/08/facebook-deep-learning.jpg\" alt=\"CNN\">\n",
        "</div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_vQt9RavKii"
      },
      "source": [
        "### Check the MaxPooling Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP4tb4oVo5oc"
      },
      "source": [
        "# load the model again\n",
        "model = VGGFace(model='resnet50')\n",
        "\n",
        "# redefine model to output right after the third hidden layer\n",
        "model = Model(inputs=model.inputs, outputs=model.layers[4].output)\n",
        "model.summary()\n",
        "# load the image with the required shape\n",
        "img = extract_face\n",
        "# convert the image to an array\n",
        "img = img_to_array(img)\n",
        "# expand dimensions so that it represents a single 'sample'\n",
        "img = expand_dims(img, axis=0)\n",
        "# prepare the image (e.g. scale pixel values for the vgg)\n",
        "img = preprocess_input(img)\n",
        "# get feature map for first hidden layer\n",
        "feature_maps = model.predict(img)\n",
        "# plot all 4 maps in an 4x4 squares\n",
        "square = 4\n",
        "ix = 1\n",
        "for _ in range(square):\n",
        "    for _ in range(square):\n",
        "        # specify subplot and turn of axis\n",
        "        ax = pyplot.subplot(square, square, ix)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        # plot filter channel in grayscale\n",
        "        pyplot.imshow(feature_maps[0, :, :, ix-1], cmap='gray')\n",
        "        ix += 1\n",
        "# show the figure\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11vi-tgbvaSz"
      },
      "source": [
        "### Check one of the Convolutional Layers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71zxSnbIo5oc"
      },
      "source": [
        "# load the model again\n",
        "model = VGGFace(model='resnet50')\n",
        "\n",
        "# redefine model to output right after the fifth hidden layer\n",
        "model = Model(inputs=model.inputs, outputs=model.layers[6].output)\n",
        "model.summary()\n",
        "# load the image with the required shape\n",
        "img = extract_face\n",
        "# convert the image to an array\n",
        "img = img_to_array(img)\n",
        "# expand dimensions so that it represents a single 'sample'\n",
        "img = expand_dims(img, axis=0)\n",
        "# prepare the image (e.g. scale pixel values for the vgg)\n",
        "img = preprocess_input(img)\n",
        "# get feature map for first hidden layer\n",
        "feature_maps = model.predict(img)\n",
        "# plot all 4 maps in an 4x4 squares\n",
        "square = 4\n",
        "ix = 1\n",
        "for _ in range(square):\n",
        "    for _ in range(square):\n",
        "        # specify subplot and turn of axis\n",
        "        ax = pyplot.subplot(square, square, ix)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        # plot filter channel in grayscale\n",
        "        pyplot.imshow(feature_maps[0, :, :, ix-1], cmap='gray')\n",
        "        ix += 1\n",
        "# show the figure\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c17AwtKZo5oc"
      },
      "source": [
        "# Step 3: Use Pre-trained Model for Your Photo Classification (Transfer Learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRD-iWHUo5oc"
      },
      "source": [
        "<div style=\"float: left; width: 85%; height: 350px; padding-bottom:280px\">\n",
        "    <img src=\"https://www.analyticssteps.com/backend/media/thumbnail/1967565/9315476_1592890541_transfer.jpg\" alt=\"CNN\">\n",
        "</div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kG9EZ2_o5od"
      },
      "source": [
        "Transfer learning is a machine learning method where a model developed for a task is reused as the starting point for a model on a second task.\n",
        "\n",
        "It is a popular approach in deep learning where pre-trained models are used as the starting point on computer vision and natural language processing tasks given the vast compute and time resources required to develop neural network models on these problems and from the huge jumps in skill that they provide on related problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mt8T1Z3vo5od"
      },
      "source": [
        "### We are going to use a public dataset for training our own model \n",
        "### VGG model did not train on these dataset before. Basically, you can train whatever dataset you want\n",
        "\n",
        "You can download the dataset from here: https://www.kaggle.com/dansbecker/5-celebrity-faces-dataset\n",
        "\n",
        "The photos haven't been cropped for consistent aspect ratios. With so few training photos, this an especially interesting test of computer vision techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWg592M2o5od"
      },
      "source": [
        "### See the sample data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9T0zod0o5od",
        "scrolled": true
      },
      "source": [
        "print(\"Loading one of the sample picture \\n\")\n",
        "\n",
        "pixels = plt.imread(\"/content/Facial-Recognition-MMAI844-Tutorial/train/ben_afflek/httpwwwhillsindcomstorebenjpg.jpg\")\n",
        "plt.imshow(pixels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV6d73nco5od"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Emv-Zicbo5od"
      },
      "source": [
        "### Step 3.1 Prepare input data and parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hzM__u3o5od"
      },
      "source": [
        "In keras there is a \"image preprocessing function\" **ImageDataGenerator** that can not only do the image preprocessing but also run data augmentation for you.\n",
        "For more information please refer to: https://keras.io/api/preprocessing/image/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfpikVKlo5od"
      },
      "source": [
        "img_height=224\n",
        "img_width=224\n",
        "batch_size=6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbbiWq0Eo5od"
      },
      "source": [
        "## Step 3.2 Specify the data folder train and val\n",
        "\n",
        "## ImageDataGenerator function can do all the image preprocessing for you !!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r7X1ET6o5od"
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1. / 255,\n",
        "    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    zoom_range = 0.1, # Randomly zoom image \n",
        "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "    #shear_range=0.2,\n",
        "    vertical_flip=False,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/Facial-Recognition-MMAI844-Tutorial/train',\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=4,\n",
        "    class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    '/content/Facial-Recognition-MMAI844-Tutorial/val',\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=4,\n",
        "    class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJC1IjAoo5od"
      },
      "source": [
        "######################################\n",
        "#Model training and inputing parameters\n",
        "#####################################\n",
        "nb_class = 5\n",
        "hidden_dim = 100\n",
        "nb_train_samples = 93\n",
        "\n",
        "nb_validation_samples = 25\n",
        "epochs = 5\n",
        "batch_size = 4\n",
        "numclasses = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFEHdikno5od"
      },
      "source": [
        "### New Model with different output layer (our own dataset is the new output layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8HqnhO_o5od"
      },
      "source": [
        "vgg_model = VGGFace(include_top=False, input_shape=(224, 224, 3))\n",
        "last_layer = vgg_model.get_layer('pool5').output\n",
        "x = Flatten(name='flatten')(last_layer)\n",
        "x = Dense(hidden_dim, activation='relu', name='fc6')(x)\n",
        "x = Dense(hidden_dim, activation='relu', name='fc7')(x)\n",
        "out = Dense(nb_class, activation='softmax', name='fc8')(x)\n",
        "custom_vgg_model = Model(vgg_model.input, out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwfFGsBYo5od",
        "scrolled": true
      },
      "source": [
        "custom_vgg_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBzkPUeno5od"
      },
      "source": [
        "lr = 1e-4\n",
        "decay = 1e-6 #0.0\n",
        "optimizer = RMSprop(lr=lr, decay=decay)\n",
        "custom_vgg_model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKplcI8Ho5od"
      },
      "source": [
        "## Step 3.3 Train model \n",
        "\n",
        "### Due to the class time constraints, we will just train couple epoch here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxMBFGgUo5oe",
        "scrolled": true
      },
      "source": [
        "history = custom_vgg_model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=nb_train_samples // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=nb_validation_samples // batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCfoXHrXo5oe"
      },
      "source": [
        "## Step 3.4 Visualize the training and validation accuracy and loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8oBXnhxo5oe"
      },
      "source": [
        "# Get training and test loss histories\n",
        "training_loss = history.history['loss']\n",
        "training_acc = history.history['accuracy']\n",
        "\n",
        "# Create count of the number of epochs\n",
        "epoch_count = range(1, len(training_loss) + 1)\n",
        "\n",
        "fig=plt.figure(figsize=(12, 4))\n",
        "# Visualize loss history\n",
        "fig.add_subplot(121)\n",
        "plt.plot(epoch_count, training_loss, 'r--')\n",
        "plt.plot(epoch_count, training_acc, 'b-')\n",
        "plt.legend(['Training Loss', 'Training Accuracy'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training Loss/Acc')\n",
        "\n",
        "# Get training and test loss histories\n",
        "val_acc = history.history['val_accuracy']\n",
        "training_acc = history.history['accuracy']\n",
        "\n",
        "# Create count of the number of epochs\n",
        "epoch_count = range(1, len(val_acc) + 1)\n",
        "\n",
        "# Visualize loss history\n",
        "fig.add_subplot(122)\n",
        "plt.plot(epoch_count, val_acc, 'r--')\n",
        "plt.plot(epoch_count, training_acc, 'b-')\n",
        "plt.legend(['Validation Accuracy', 'Training Accuracy'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mLt-KwIo5oe"
      },
      "source": [
        "## Step 3.5 Save the model and run prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtWoAUbao5oe"
      },
      "source": [
        "saveweight =  'celebriytag_weight.h5'\n",
        "model.save_weights(saveweight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRi5G7Xio5oe"
      },
      "source": [
        "labels = ['ben_afflek',  'elton_john',  'jerry_seinfeld',  'madonna',  'mindy_kaling']\n",
        "test_imgs = ['/content/Facial-Recognition-MMAI844-Tutorial/val/ben_afflek/123MTENDgMDUODczNDcNTcjpg.jpg']\n",
        "\n",
        "\n",
        "test_img = '/content/Facial-Recognition-MMAI844-Tutorial/val/ben_afflek/123MTENDgMDUODczNDcNTcjpg.jpg'\n",
        "img = load_img(test_img, target_size=(img_width, img_height))\n",
        "x = img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x /= 255.\n",
        "classes = custom_vgg_model.predict(x)\n",
        "result = np.squeeze(classes)\n",
        "result_indices = np.argmax(result)\n",
        "    \n",
        "img = cv2.imread(test_img, cv2.IMREAD_COLOR)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "plt.axis('off')\n",
        "plt.title(\"{}, {:.2f}%\".format(labels[result_indices], result[result_indices]*100))\n",
        "plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqdQOAe6o5oe"
      },
      "source": [
        "## Hope you enjoyed the tutorial\n",
        "### If you have any question about the notebook, please contact:  \n",
        "\n",
        "### GROUP-Albert_MMAI2021 \n",
        "GROUP-Albert_MMAI2021@queensu.ca\n",
        "\n",
        "### Bencheng Wei \n",
        "20bw3@queensu.ca "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_CshaaOo5oe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}